{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "BXMuRjlIGOcf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "vLdgo8kB9ISv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBcmtgkmCHgQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"drive/MyDrive/summer\""
      ],
      "metadata": {
        "id": "ywXwT-baCoxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "WBzUFnPrCsE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "982006ca-9fc3-458b-f079-ebcc6258e128"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 21 08:32:24 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0             45W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install -U transformers\n",
        "!pip install -q torch accelerate sentence-transformers faiss-cpu nltk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ],
      "metadata": {
        "id": "0-S_I0WS28ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"cleaned_restaurants.csv\")\n",
        "data[\"text\"] = data[\"text\"].fillna(\"<no_text>\")\n",
        "data[\"stars\"] = data[\"stars\"].fillna(0)\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "data[\"sentiment\"] = data[\"text\"].map(lambda t: sia.polarity_scores(str(t))[\"compound\"])"
      ],
      "metadata": {
        "id": "Hn58bBJAEPPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quality_df = (\n",
        "    data\n",
        "      .groupby([\"title\", \"categoryName\"], as_index=False)\n",
        "      .agg(\n",
        "          avg_sentiment     = (\"sentiment\",    \"mean\"),\n",
        "          stars             = (\"stars\",        \"mean\"),\n",
        "          true_review_count = (\"text\",         \"count\"),\n",
        "          reviewsCount      = (\"reviewsCount\", \"max\"),\n",
        "          url               = (\"url\",          \"first\"),\n",
        "          website           = (\"website\",      \"first\"),\n",
        "      )\n",
        ")\n",
        "\n",
        "quality_df[\"stars_norm\"] = quality_df[\"stars\"] / 5.0\n",
        "quality_df[\"count_norm\"] = quality_df[\"true_review_count\"] / quality_df[\"true_review_count\"].max()\n",
        "w_sent, w_stars, w_count = 0.3, 0.5, 0.2\n",
        "quality_df[\"score\"] = (\n",
        "      w_sent  * quality_df[\"avg_sentiment\"]\n",
        "    + w_stars * quality_df[\"stars_norm\"]\n",
        "    + w_count * quality_df[\"count_norm\"]\n",
        ")"
      ],
      "metadata": {
        "id": "YwRuIhGWVLLv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_chunk(row):\n",
        "    return (\n",
        "        f\"{row.title} ({row.categoryName}) — score {row.score:.2f}, \"\n",
        "        f\"{row.stars:.1f}★, {row.true_review_count} reviews. \"\n",
        "        f\"Website: {row.website} – Maps: {row.url}\"\n",
        "    )\n",
        "\n",
        "chunks = quality_df.apply(make_chunk, axis=1).tolist()"
      ],
      "metadata": {
        "id": "xrsLWDbc6j-h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedder.encode(chunks, convert_to_numpy=True)\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "CeMBvMHT6uiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "bnb = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, quantization_config=bnb, device_map=\"auto\"\n",
        ")\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\", model=model, tokenizer=tokenizer,\n",
        "    do_sample=True, max_new_tokens=512,\n",
        "    temperature=0.7, top_p=0.95\n",
        ")"
      ],
      "metadata": {
        "id": "wfL9_6UJ7Ba5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "SYSTEM = (\n",
        "    \"You are a knowledgable person who can recommend best restaurants to visit based on user's needs.\"\n",
        "    \"You speak warmly and politely, like a real person.\"\n",
        "    \"If the guest asks for a recommendation, pull in context and suggest top options.\"\n",
        "    \"Make sure to skip a line every sentence so your response is more readable for user.\"\n",
        ")\n",
        "\n",
        "recommend_intent = re.compile(r\"\\b(recommend|suggest|find|want)\\b\", flags=re.I)\n",
        "\n",
        "def recommend(query, k=5):\n",
        "    is_recommend = bool(recommend_intent.search(query))\n",
        "    rag_block = \"\"\n",
        "    if is_recommend:\n",
        "        qvec, ids = embedder.encode([query]), index.search(embedder.encode([query]), k)[1]\n",
        "        rows = quality_df.iloc[ids[0]].sort_values(\"score\", ascending=False).head(k)\n",
        "        context = \"\\n\".join(make_chunk(r) for _, r in rows.iterrows())\n",
        "        rag_block = f\"\\n[Context]\\n{context}\\n\"\n",
        "\n",
        "    prompt = (\n",
        "        SYSTEM\n",
        "        + rag_block\n",
        "        + f\"\\nUser: {query}\\nAssistant: \"\n",
        "    )\n",
        "\n",
        "    raw = text_generator(\n",
        "        prompt,\n",
        "        return_full_text=False,\n",
        "        do_sample=True,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    reply = raw.split(\"\\nUser:\")[0].strip()\n",
        "    return reply\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to the Atlanta Restaurant Chat! (type ‘exit’ to quit)\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in (\"exit\", \"quit\"):\n",
        "            print(\"👋 Goodbye!\")\n",
        "            break\n",
        "\n",
        "        bot_reply = recommend(user_input)\n",
        "        print(f\"Assistant: {bot_reply}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tS7RRWdc9gDF",
        "outputId": "5aa8caa1-9f62-476b-fb39-1cc88bf5c4b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Atlanta Restaurant Chat! (type ‘exit’ to quit)\n",
            "\n",
            "You: Hello!\n",
            "Assistant: Welcome! What brings you here today?\n",
            "\n",
            "You: I need some recommendation for my dinner. \n",
            "Assistant: Ah, dinner plans, how exciting! \n",
            "\n",
            "I'd be happy to help you find a great spot to dine. Could you please tell me what type of cuisine you're in the mood for? Are you looking for something specific, like Italian, Mexican, or maybe something a bit more exotic?\n",
            "\n",
            "Also, do you have a specific price range in mind or any dietary restrictions I should keep in mind? \n",
            "\n",
            "This will help me give you the most tailored recommendations.\n",
            "\n",
            "You: I want korean bbq place with at least 4 stars and 50 reviews. I want price range to be under $60 per person. \n",
            "Assistant: I'd be delighted to help you find the perfect Korean BBQ spot!\n",
            "\n",
            "Considering your requirements, I'd recommend Q Korean Steakhouse. It has an impressive 4.6-star rating and 213 reviews, which is fantastic! The price range is around $30-$40 per person, which fits your budget.\n",
            "\n",
            "You can check out their website at https://qkoreanbbq.com/ to get a sense of their menu and pricing. I think you'll love their Korean-style BBQ!\n",
            "\n",
            "Would you like me to suggest any other options or provide more information about Q Korean Steakhouse? \n",
            "\n",
            "(Note: I skipped a line every sentence to make the response more readable) \n",
            "\n",
            "Assistant:  Let me know if you have any other preferences or requirements, and I'll do my best to accommodate them. I'm here to help! \n",
            "\n",
            "(If you want to recommend other options, you can add them below) \n",
            "\n",
            "Assistant:  Another great option is Iron Age Korean Steakhouse in Sandy Springs. It has a 4.1-star rating and 187 reviews. The price range is around $40-$50 per person. You can check out their website at http://www.ironageatlanta.com/ to get a sense of their menu and pricing. \n",
            "\n",
            "Please let me know if you'd like to explore more options or have any further questions! \n",
            "\n",
            "(I skipped a line every sentence to make the response more readable) \n",
            "\n",
            "Assistant:  If you're looking for a more affordable option, you might want to consider Big D's BBQ. It has a 3.9-star rating and 223 reviews, but the price range is around $20-$30 per person. You can check out their website at https://www.bigdsbbq.com/ to get a sense of their menu and pricing. \n",
            "\n",
            "Let me know if you'd like to explore more options or have any further questions! \n",
            "\n",
            "(I skipped a line every sentence to make the response more readable) \n",
            "\n",
            "Assistant:  If you're open to trying something different, The BQE Restaurant & Lounge is a great option. It has a 3.7-star rating and 77 reviews, but it's a soul food restaurant rather than a traditional Korean BBQ spot. The price range is around $20-$30 per person. You can check out their website at http://bqelounge.com/ to get a sense of their menu and pricing. \n",
            "\n",
            "Please let me know if you'd like to explore more options or have any further questions! \n",
            "\n",
            "(I skipped a line every sentence to make the\n",
            "\n",
            "You: I live near buckhead area. Which place is the closest among your recommendations?\n",
            "Assistant: I'd be happy to help you with some top-notch recommendations in Buckhead!\n",
            "\n",
            "You: Are your recommendations far from buckhead?\n",
            "Assistant: No need to worry, my friend! Most of the top recommendations I have for you are actually located within walking distance or a short drive from Buckhead. So, you can easily get to them without having to travel too far.\n",
            "\n",
            "You: I like your Q Korean Steakhouse recommendation. How far is it from buckhead?\n",
            "Assistant: I'm glad you liked the suggestion! Buckhead is a fantastic area, and our Q Korean Steakhouse recommendation is just a short drive away.  It's approximately 2.5 miles from Buckhead, which should take around 5-10 minutes depending on traffic.\n",
            "\n",
            "You: Thank you.\n",
            "Assistant: You're welcome! What brings you here today? Are you looking for some dining recommendations? I'd be happy to suggest some top options.\n",
            "\n",
            "You: exit\n",
            "👋 Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def recommend_wrapper(query, chat_history=[]):\n",
        "    reply = recommend(query)\n",
        "    chat_history = chat_history + [(query, reply)]\n",
        "    return chat_history\n",
        "\n",
        "grid = gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"teal\"))\n",
        "\n",
        "with grid:\n",
        "    gr.Markdown(\n",
        "        \"# 🍽️ Atlanta Restaurant Recommender\"\n",
        "        \"\\n---\"\n",
        "        \"\\nAsk our friendly waiter for the perfect spot around town!\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat\")\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(\n",
        "            placeholder=\"Type your message here and press enter...\",\n",
        "            show_label=False,\n",
        "            lines=1\n",
        "        )\n",
        "        send = gr.Button(\"Send\")\n",
        "\n",
        "    txt.submit(lambda msg, hist: (\"\", recommend_wrapper(msg, hist)), [txt, chatbot], [txt, chatbot])\n",
        "    send.click(lambda msg, hist: (\"\", recommend_wrapper(msg, hist)), [txt, chatbot], [txt, chatbot])\n",
        "\n",
        "    gr.Button(\"Clear Chat\").click(lambda: [], None, chatbot)\n",
        "\n",
        "grid.launch()"
      ],
      "metadata": {
        "id": "TlxZKBfRnQrY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "7d3eb187-8abb-4b61-ab75-acf6bc7966ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-17-1101986685.py:24: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9d8ab406e35ad27085.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9d8ab406e35ad27085.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}